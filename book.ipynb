{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some corrections (bad columns' names, tabulation, ...) have been done to the data before loading it\n",
    "# During exporting, we let Python choose the right data types for each column for now...\n",
    "data = pd.read_csv(r\"books_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore data for more corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11127, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows, and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookID</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>isbn</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>language_code</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>J.K. Rowling / Mary GrandPré</td>\n",
       "      <td>4.57</td>\n",
       "      <td>439785960</td>\n",
       "      <td>9.780000e+12</td>\n",
       "      <td>eng</td>\n",
       "      <td>652</td>\n",
       "      <td>2095690</td>\n",
       "      <td>27591</td>\n",
       "      <td>9/16/2006</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>J.K. Rowling / Mary GrandPré</td>\n",
       "      <td>4.49</td>\n",
       "      <td>439358078</td>\n",
       "      <td>9.780000e+12</td>\n",
       "      <td>eng</td>\n",
       "      <td>870</td>\n",
       "      <td>2153167</td>\n",
       "      <td>29221</td>\n",
       "      <td>9/1/2004</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bookID                                              title  \\\n",
       "0       1  Harry Potter and the Half-Blood Prince (Harry ...   \n",
       "1       2  Harry Potter and the Order of the Phoenix (Har...   \n",
       "\n",
       "                        authors  average_rating       isbn        isbn13  \\\n",
       "0  J.K. Rowling / Mary GrandPré            4.57  439785960  9.780000e+12   \n",
       "1  J.K. Rowling / Mary GrandPré            4.49  439358078  9.780000e+12   \n",
       "\n",
       "  language_code  num_pages  ratings_count  text_reviews_count  \\\n",
       "0           eng        652        2095690               27591   \n",
       "1           eng        870        2153167               29221   \n",
       "\n",
       "  publication_date        publisher  \n",
       "0        9/16/2006  Scholastic Inc.  \n",
       "1         9/1/2004  Scholastic Inc.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View of the first two rows\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11127 entries, 0 to 11126\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   bookID              11127 non-null  int64  \n",
      " 1   title               11127 non-null  object \n",
      " 2   authors             11127 non-null  object \n",
      " 3   average_rating      11127 non-null  float64\n",
      " 4   isbn                11127 non-null  object \n",
      " 5   isbn13              11127 non-null  float64\n",
      " 6   language_code       11127 non-null  object \n",
      " 7   num_pages           11127 non-null  int64  \n",
      " 8   ratings_count       11127 non-null  int64  \n",
      " 9   text_reviews_count  11127 non-null  int64  \n",
      " 10  publication_date    11127 non-null  object \n",
      " 11  publisher           11127 non-null  object \n",
      "dtypes: float64(2), int64(4), object(6)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# View of the data types chosen for each column by Python, and eventually number of NA (missing data)\n",
    "data.info()\n",
    "# The data types chosen for each column is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookID</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11127.000000</td>\n",
       "      <td>11127.000000</td>\n",
       "      <td>1.112700e+04</td>\n",
       "      <td>11127.000000</td>\n",
       "      <td>1.112700e+04</td>\n",
       "      <td>11127.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>21310.938887</td>\n",
       "      <td>3.933631</td>\n",
       "      <td>9.759178e+12</td>\n",
       "      <td>336.376921</td>\n",
       "      <td>1.793641e+04</td>\n",
       "      <td>541.854498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13093.358023</td>\n",
       "      <td>0.352445</td>\n",
       "      <td>4.428650e+11</td>\n",
       "      <td>241.127305</td>\n",
       "      <td>1.124794e+05</td>\n",
       "      <td>2576.176608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.987060e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10287.000000</td>\n",
       "      <td>3.770000</td>\n",
       "      <td>9.780000e+12</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>1.040000e+02</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20287.000000</td>\n",
       "      <td>3.960000</td>\n",
       "      <td>9.780000e+12</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>7.450000e+02</td>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32104.500000</td>\n",
       "      <td>4.135000</td>\n",
       "      <td>9.780000e+12</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>4.993500e+03</td>\n",
       "      <td>237.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>45641.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.790000e+12</td>\n",
       "      <td>6576.000000</td>\n",
       "      <td>4.597666e+06</td>\n",
       "      <td>94265.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             bookID  average_rating        isbn13     num_pages  \\\n",
       "count  11127.000000    11127.000000  1.112700e+04  11127.000000   \n",
       "mean   21310.938887        3.933631  9.759178e+12    336.376921   \n",
       "std    13093.358023        0.352445  4.428650e+11    241.127305   \n",
       "min        1.000000        0.000000  8.987060e+09      0.000000   \n",
       "25%    10287.000000        3.770000  9.780000e+12    192.000000   \n",
       "50%    20287.000000        3.960000  9.780000e+12    299.000000   \n",
       "75%    32104.500000        4.135000  9.780000e+12    416.000000   \n",
       "max    45641.000000        5.000000  9.790000e+12   6576.000000   \n",
       "\n",
       "       ratings_count  text_reviews_count  \n",
       "count   1.112700e+04        11127.000000  \n",
       "mean    1.793641e+04          541.854498  \n",
       "std     1.124794e+05         2576.176608  \n",
       "min     0.000000e+00            0.000000  \n",
       "25%     1.040000e+02            9.000000  \n",
       "50%     7.450000e+02           46.000000  \n",
       "75%     4.993500e+03          237.500000  \n",
       "max     4.597666e+06        94265.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()\n",
    "# We can't have average_rate, num_pages or ratings_count equal to zero,\n",
    "# we must investigate and eventually drop the corresponding rows, they can be outliers ;\n",
    "\n",
    "# But text_reviews_count can be equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>language_code</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>J.K. Rowling / Mary GrandPré</td>\n",
       "      <td>4.57</td>\n",
       "      <td>eng</td>\n",
       "      <td>652</td>\n",
       "      <td>2095690</td>\n",
       "      <td>27591</td>\n",
       "      <td>9/16/2006</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>J.K. Rowling / Mary GrandPré</td>\n",
       "      <td>4.49</td>\n",
       "      <td>eng</td>\n",
       "      <td>870</td>\n",
       "      <td>2153167</td>\n",
       "      <td>29221</td>\n",
       "      <td>9/1/2004</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Harry Potter and the Half-Blood Prince (Harry ...   \n",
       "1  Harry Potter and the Order of the Phoenix (Har...   \n",
       "\n",
       "                        authors  average_rating language_code  num_pages  \\\n",
       "0  J.K. Rowling / Mary GrandPré            4.57           eng        652   \n",
       "1  J.K. Rowling / Mary GrandPré            4.49           eng        870   \n",
       "\n",
       "   ratings_count  text_reviews_count publication_date        publisher  \n",
       "0        2095690               27591        9/16/2006  Scholastic Inc.  \n",
       "1        2153167               29221         9/1/2004  Scholastic Inc.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = [\"bookID\", \"isbn\", \"isbn13\"]\n",
    "\n",
    "# We decided to drop these columns because they are IDs and they don't really determine the average rating: they are just identifiers\n",
    "data1 = data.drop(columns_to_drop, axis=1, inplace=False)\n",
    "data1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>language_code</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>J.K. Rowling / Mary GrandPré</td>\n",
       "      <td>4.57</td>\n",
       "      <td>eng</td>\n",
       "      <td>652</td>\n",
       "      <td>2095690</td>\n",
       "      <td>27591</td>\n",
       "      <td>2006</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>J.K. Rowling / Mary GrandPré</td>\n",
       "      <td>4.49</td>\n",
       "      <td>eng</td>\n",
       "      <td>870</td>\n",
       "      <td>2153167</td>\n",
       "      <td>29221</td>\n",
       "      <td>2004</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Harry Potter and the Half-Blood Prince (Harry ...   \n",
       "1  Harry Potter and the Order of the Phoenix (Har...   \n",
       "\n",
       "                        authors  average_rating language_code  num_pages  \\\n",
       "0  J.K. Rowling / Mary GrandPré            4.57           eng        652   \n",
       "1  J.K. Rowling / Mary GrandPré            4.49           eng        870   \n",
       "\n",
       "   ratings_count  text_reviews_count  publication_year        publisher  \n",
       "0        2095690               27591              2006  Scholastic Inc.  \n",
       "1        2153167               29221              2004  Scholastic Inc.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the publication year, instead of having the long date (this format of long date isn't really important)\n",
    "data1[\"publication_date\"] = data1[\"publication_date\"].apply(lambda x: int(x.split(\"/\")[-1]))\n",
    "data1.rename(columns={\"publication_date\" : \"publication_year\"}, inplace=True)\n",
    "data1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language_code\n",
       "eng      8911\n",
       "en-US    1409\n",
       "spa       218\n",
       "en-GB     214\n",
       "fre       144\n",
       "ger        99\n",
       "jpn        46\n",
       "mul        19\n",
       "zho        14\n",
       "grc        11\n",
       "por        10\n",
       "en-CA       7\n",
       "ita         5\n",
       "enm         3\n",
       "lat         3\n",
       "swe         2\n",
       "rus         2\n",
       "srp         1\n",
       "nl          1\n",
       "msa         1\n",
       "glg         1\n",
       "wel         1\n",
       "ara         1\n",
       "nor         1\n",
       "tur         1\n",
       "gla         1\n",
       "ale         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the langage code, by proceeding with data1\n",
    "data1[\"language_code\"].value_counts()\n",
    "# Books written in english were the most rated, but there are also some langages (like arabic, turkish) in minority..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop language_code\n",
    "del data1[\"language_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11127.000000</td>\n",
       "      <td>11127.000000</td>\n",
       "      <td>1.112700e+04</td>\n",
       "      <td>11127.000000</td>\n",
       "      <td>11127.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.933631</td>\n",
       "      <td>336.376921</td>\n",
       "      <td>1.793641e+04</td>\n",
       "      <td>541.854498</td>\n",
       "      <td>2000.167520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.352445</td>\n",
       "      <td>241.127305</td>\n",
       "      <td>1.124794e+05</td>\n",
       "      <td>2576.176608</td>\n",
       "      <td>8.248836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.770000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>1.040000e+02</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1998.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.960000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>7.450000e+02</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>2003.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.135000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>4.993500e+03</td>\n",
       "      <td>237.500000</td>\n",
       "      <td>2005.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>6576.000000</td>\n",
       "      <td>4.597666e+06</td>\n",
       "      <td>94265.000000</td>\n",
       "      <td>2020.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       average_rating     num_pages  ratings_count  text_reviews_count  \\\n",
       "count    11127.000000  11127.000000   1.112700e+04        11127.000000   \n",
       "mean         3.933631    336.376921   1.793641e+04          541.854498   \n",
       "std          0.352445    241.127305   1.124794e+05         2576.176608   \n",
       "min          0.000000      0.000000   0.000000e+00            0.000000   \n",
       "25%          3.770000    192.000000   1.040000e+02            9.000000   \n",
       "50%          3.960000    299.000000   7.450000e+02           46.000000   \n",
       "75%          4.135000    416.000000   4.993500e+03          237.500000   \n",
       "max          5.000000   6576.000000   4.597666e+06        94265.000000   \n",
       "\n",
       "       publication_year  \n",
       "count      11127.000000  \n",
       "mean        2000.167520  \n",
       "std            8.248836  \n",
       "min         1900.000000  \n",
       "25%         1998.000000  \n",
       "50%         2003.000000  \n",
       "75%         2005.000000  \n",
       "max         2020.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9375</th>\n",
       "      <td>Consider the Lilies</td>\n",
       "      <td>Iain Crichton Smith / Isobel Murray</td>\n",
       "      <td>3.88</td>\n",
       "      <td>144</td>\n",
       "      <td>332</td>\n",
       "      <td>33</td>\n",
       "      <td>1900</td>\n",
       "      <td>Polygon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title                              authors  \\\n",
       "9375  Consider the Lilies  Iain Crichton Smith / Isobel Murray   \n",
       "\n",
       "      average_rating  num_pages  ratings_count  text_reviews_count  \\\n",
       "9375            3.88        144            332                  33   \n",
       "\n",
       "      publication_year publisher  \n",
       "9375              1900   Polygon  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate the oldest book (written in 1900)\n",
    "data1[data1[\"publication_year\"]==1900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publication_year\n",
       "2006    1700\n",
       "2005    1260\n",
       "2004    1071\n",
       "2003     931\n",
       "2002     798\n",
       "        ... \n",
       "1947       1\n",
       "1922       1\n",
       "1929       1\n",
       "1919       1\n",
       "1940       1\n",
       "Name: count, Length: 87, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate the publication_year distibution\n",
    "data1[\"publication_year\"].value_counts()\n",
    "\n",
    "# It seems that most of the rated books are recent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2006, 2004, 2003, 2005, 2002, 1996, 2001, 2000, 1990, 1993, 1997,\n",
       "       1991, 2012, 1994, 1982, 1986, 1999, 1987, 1984, 2007, 1965, 1998,\n",
       "       1995, 1964, 1970, 2011, 1955, 1988, 1985, 1989, 1963, 2008, 2009,\n",
       "       1976, 1975, 1980, 1992, 1973, 2019, 1954, 2015, 1919, 1921, 1923,\n",
       "       1969, 1968, 1961, 1953, 1958, 1983, 1978, 1929, 1977, 1979, 1922,\n",
       "       1981, 2010, 1950, 1971, 1960, 1959, 2013, 2017, 2016, 1972, 1947,\n",
       "       1943, 1974, 1957, 2014, 2018, 1952, 1935, 1956, 1966, 1925, 1962,\n",
       "       1949, 1913, 1928, 1914, 1948, 1967, 1900, 2020, 1931, 1940],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[\"publication_year\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11127, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows, and columns (columns'number must change since we delete \"bookID\", \"isbn\", \"language_code\" and \"isbn13\")\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                 10352\n",
       "authors                6643\n",
       "average_rating          209\n",
       "num_pages               997\n",
       "ratings_count          5294\n",
       "text_reviews_count     1822\n",
       "publication_year         87\n",
       "publisher              2292\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the number of unique values for each column\n",
    "data1.nunique()\n",
    "\n",
    "# The columns title and authors which are categorical variables have the most different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Be sure that none row has missing year\n",
    "data1[\"publication_year\"].isnull().sum()\n",
    "# data[\"publication_year\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Out to Eat London 2002 (Lonely Planet Out to Eat)</td>\n",
       "      <td>Lonely Planet / Mark Honan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>295</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001</td>\n",
       "      <td>Lonely Planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Juiced Official Strategy Guide</td>\n",
       "      <td>Doug Walsh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>BradyGames</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>Open City 6: The Only Woman He Ever Left</td>\n",
       "      <td>Open City Magazine / James Purdy / Daniel Pinc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>Grove Press  Open City Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>How To Meditate: An Anthology Of Talks On Medi...</td>\n",
       "      <td>Frederick P. Lenz</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>Frederick P. Lenz Foundation for American Budd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>Insights: Talks On The Nature Of Existence</td>\n",
       "      <td>Frederick P. Lenz</td>\n",
       "      <td>0.0</td>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>Frederick P. Lenz Foundation for American Budd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>Venac sonetnih venaca; Puževa srma</td>\n",
       "      <td>Dobrica Erić</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1996</td>\n",
       "      <td>Izdavačka agencija \"Draganić\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3493</th>\n",
       "      <td>Brodie's notes on Aldous Huxley's brave new world</td>\n",
       "      <td>Graham Handley</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>Macmillan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4242</th>\n",
       "      <td>American Film Guide</td>\n",
       "      <td>Frank N. Magill</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1983</td>\n",
       "      <td>Salem Press Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>The Man and the Author: John Milton: Twentieth...</td>\n",
       "      <td>J. Martin Evans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Routledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5325</th>\n",
       "      <td>Canopy: A Work for Voice and Light in Harvard ...</td>\n",
       "      <td>David   Ward / Parveen Adams / Seamus Heaney /...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1997</td>\n",
       "      <td>Arts Publications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6383</th>\n",
       "      <td>Laguna  I Love You: The Best of \"Our Town\"</td>\n",
       "      <td>John Weld / Phil Interlandi</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1996</td>\n",
       "      <td>Fithian Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6561</th>\n",
       "      <td>American Writers  Supplement VIII</td>\n",
       "      <td>Jay Parini / August Wilson</td>\n",
       "      <td>0.0</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001</td>\n",
       "      <td>Gale Cengage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6862</th>\n",
       "      <td>The Road To War  1933 39</td>\n",
       "      <td>Andrew      Hunt</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>Hodder &amp; Stoughton Educational Division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6880</th>\n",
       "      <td>I'll Be Home Before Midnight and I Won't Get P...</td>\n",
       "      <td>Anthony E. Wolf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1988</td>\n",
       "      <td>Knopf Doubleday Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7147</th>\n",
       "      <td>Mythographi Graeci 1: Apollodori Bibliotheca  ...</td>\n",
       "      <td>Apollodorus / Richard Wagner</td>\n",
       "      <td>0.0</td>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>K.G. Saur Verlag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7402</th>\n",
       "      <td>Sclerotherapy and vein treatment</td>\n",
       "      <td>Robert A. Weiss / Margaret A. Weiss / Karen L....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>248</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011</td>\n",
       "      <td>McGraw-Hill Professional Publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7576</th>\n",
       "      <td>The Berlin Phenomenology</td>\n",
       "      <td>Georg Wilhelm Friedrich Hegel / Michael John P...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1981</td>\n",
       "      <td>Springer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7637</th>\n",
       "      <td>Your Child and Jesus: A Family Activity Book</td>\n",
       "      <td>Rick Osborne / Kevin Miller</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>Moody Publishers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7800</th>\n",
       "      <td>Beckett Football Card Price Guide</td>\n",
       "      <td>Dan Hitt / James Beckett III</td>\n",
       "      <td>0.0</td>\n",
       "      <td>830</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>Beckett Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8979</th>\n",
       "      <td>Brown's Star Atlas: Showing All The Bright Sta...</td>\n",
       "      <td>Brown Son &amp; Ferguson</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1977</td>\n",
       "      <td>Brown Son &amp; Ferguson Ltd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9335</th>\n",
       "      <td>Random House Crossword Mega Omnibus  Volume 1</td>\n",
       "      <td>United Feature Syndication</td>\n",
       "      <td>0.0</td>\n",
       "      <td>336</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1997</td>\n",
       "      <td>Random House Puzzles &amp; Games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9441</th>\n",
       "      <td>Classroom Interactions as Cross-Cultural Encou...</td>\n",
       "      <td>Jasmine C.M. Luk / Angel M.Y. Lin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>Routledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Day and Night</td>\n",
       "      <td>Better Homes and Gardens</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1989</td>\n",
       "      <td>Meredith Corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>The Fantastic Vampire: Studies in the Children...</td>\n",
       "      <td>James Craig Holte</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Greenwood Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10921</th>\n",
       "      <td>The Leadership Challenge: Skills for Taking Ch...</td>\n",
       "      <td>Warren G. Bennis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>AMR/Advanced Management Reports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11031</th>\n",
       "      <td>Anthony Thwaite: In Conversation With Peter Da...</td>\n",
       "      <td>Peter  Dale / Ian          Hamilton / Anthony ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>Between the Lines Productions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "265    Out to Eat London 2002 (Lonely Planet Out to Eat)   \n",
       "375                       Juiced Official Strategy Guide   \n",
       "987             Open City 6: The Only Woman He Ever Left   \n",
       "2532   How To Meditate: An Anthology Of Talks On Medi...   \n",
       "2533          Insights: Talks On The Nature Of Existence   \n",
       "2758                 Venac sonetnih venaca; Puževa srma   \n",
       "3493   Brodie's notes on Aldous Huxley's brave new world   \n",
       "4242                                 American Film Guide   \n",
       "4678   The Man and the Author: John Milton: Twentieth...   \n",
       "5325   Canopy: A Work for Voice and Light in Harvard ...   \n",
       "6383          Laguna  I Love You: The Best of \"Our Town\"   \n",
       "6561                   American Writers  Supplement VIII   \n",
       "6862                            The Road To War  1933 39   \n",
       "6880   I'll Be Home Before Midnight and I Won't Get P...   \n",
       "7147   Mythographi Graeci 1: Apollodori Bibliotheca  ...   \n",
       "7402                    Sclerotherapy and vein treatment   \n",
       "7576                            The Berlin Phenomenology   \n",
       "7637        Your Child and Jesus: A Family Activity Book   \n",
       "7800                   Beckett Football Card Price Guide   \n",
       "8979   Brown's Star Atlas: Showing All The Bright Sta...   \n",
       "9335       Random House Crossword Mega Omnibus  Volume 1   \n",
       "9441   Classroom Interactions as Cross-Cultural Encou...   \n",
       "10142                                      Day and Night   \n",
       "10501  The Fantastic Vampire: Studies in the Children...   \n",
       "10921  The Leadership Challenge: Skills for Taking Ch...   \n",
       "11031  Anthony Thwaite: In Conversation With Peter Da...   \n",
       "\n",
       "                                                 authors  average_rating  \\\n",
       "265                           Lonely Planet / Mark Honan             0.0   \n",
       "375                                           Doug Walsh             0.0   \n",
       "987    Open City Magazine / James Purdy / Daniel Pinc...             0.0   \n",
       "2532                                   Frederick P. Lenz             0.0   \n",
       "2533                                   Frederick P. Lenz             0.0   \n",
       "2758                                        Dobrica Erić             0.0   \n",
       "3493                                      Graham Handley             0.0   \n",
       "4242                                     Frank N. Magill             0.0   \n",
       "4678                                     J. Martin Evans             0.0   \n",
       "5325   David   Ward / Parveen Adams / Seamus Heaney /...             0.0   \n",
       "6383                         John Weld / Phil Interlandi             0.0   \n",
       "6561                          Jay Parini / August Wilson             0.0   \n",
       "6862                                    Andrew      Hunt             0.0   \n",
       "6880                                     Anthony E. Wolf             0.0   \n",
       "7147                        Apollodorus / Richard Wagner             0.0   \n",
       "7402   Robert A. Weiss / Margaret A. Weiss / Karen L....             0.0   \n",
       "7576   Georg Wilhelm Friedrich Hegel / Michael John P...             0.0   \n",
       "7637                         Rick Osborne / Kevin Miller             0.0   \n",
       "7800                        Dan Hitt / James Beckett III             0.0   \n",
       "8979                                Brown Son & Ferguson             0.0   \n",
       "9335                          United Feature Syndication             0.0   \n",
       "9441                   Jasmine C.M. Luk / Angel M.Y. Lin             0.0   \n",
       "10142                           Better Homes and Gardens             0.0   \n",
       "10501                                  James Craig Holte             0.0   \n",
       "10921                                   Warren G. Bennis             0.0   \n",
       "11031  Peter  Dale / Ian          Hamilton / Anthony ...             0.0   \n",
       "\n",
       "       num_pages  ratings_count  text_reviews_count  publication_year  \\\n",
       "265          295              0                   0              2001   \n",
       "375          112              0                   0              2005   \n",
       "987          200              0                   0              2000   \n",
       "2532         228              0                   0              2004   \n",
       "2533         304              0                   0              2003   \n",
       "2758         222              0                   0              1996   \n",
       "3493          71              0                   0              1992   \n",
       "4242           5              0                   0              1983   \n",
       "4678         386              0                   0              2002   \n",
       "5325          63              0                   0              1997   \n",
       "6383         285              0                   0              1996   \n",
       "6561         400              0                   0              2001   \n",
       "6862         128              0                   0              2000   \n",
       "6880         288              0                   0              1988   \n",
       "7147         335              0                   0              1998   \n",
       "7402         248              0                   0              2011   \n",
       "7576         210              0                   0              1981   \n",
       "7637         112              0                   0              1999   \n",
       "7800         830              0                   0              2006   \n",
       "8979          49              0                   0              1977   \n",
       "9335         336              0                   0              1997   \n",
       "9441         241              0                   0              2006   \n",
       "10142         32              0                   1              1989   \n",
       "10501        176              0                   0              2002   \n",
       "10921         60              0                   0              1985   \n",
       "11031         96              0                   0              1999   \n",
       "\n",
       "                                               publisher  \n",
       "265                                        Lonely Planet  \n",
       "375                                           BradyGames  \n",
       "987                         Grove Press  Open City Books  \n",
       "2532   Frederick P. Lenz Foundation for American Budd...  \n",
       "2533   Frederick P. Lenz Foundation for American Budd...  \n",
       "2758                      Izdavačka agencija \"Draganić\"  \n",
       "3493                                           Macmillan  \n",
       "4242                                     Salem Press Inc  \n",
       "4678                                           Routledge  \n",
       "5325                                   Arts Publications  \n",
       "6383                                       Fithian Press  \n",
       "6561                                        Gale Cengage  \n",
       "6862             Hodder & Stoughton Educational Division  \n",
       "6880                    Knopf Doubleday Publishing Group  \n",
       "7147                                    K.G. Saur Verlag  \n",
       "7402                 McGraw-Hill Professional Publishing  \n",
       "7576                                            Springer  \n",
       "7637                                    Moody Publishers  \n",
       "7800                                       Beckett Media  \n",
       "8979                           Brown Son & Ferguson Ltd.  \n",
       "9335                        Random House Puzzles & Games  \n",
       "9441                                           Routledge  \n",
       "10142                               Meredith Corporation  \n",
       "10501                                    Greenwood Press  \n",
       "10921                    AMR/Advanced Management Reports  \n",
       "11031                      Between the Lines Productions  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the rows with average_rating == 0\n",
    "data1[data1[\"average_rating\"]==0]\n",
    "\n",
    "# In generally, the rows with average_rating == 0, have also ratings_count == 0\n",
    "# That's normal since there weren't reviewed by people, so 0 is a default value in these cases.\n",
    "# This 0 can be seen like some kind of default value for books which haven't be reviewed.\n",
    "\n",
    "# Keeping these rows can be problematic !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11101, 8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the rows having average_rating == 0\n",
    "data2 = data1[~(data1[\"average_rating\"]==0)]\n",
    "# data2 contains only rows with average_rating != 0\n",
    "\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Lonely Planet Londres</td>\n",
       "      <td>Lonely Planet / Sarah Johnstone / Tom Masters</td>\n",
       "      <td>4.03</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>Geoplaneta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>American Government: Continuity and Change  Al...</td>\n",
       "      <td>Karen  O'Connor / Larry J. Sabato</td>\n",
       "      <td>2.83</td>\n",
       "      <td>664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>Longman Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>Essentials of American and Texas Government: C...</td>\n",
       "      <td>Karen  O'Connor / Larry J. Sabato</td>\n",
       "      <td>3.50</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>Longman Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Comoediae 1: Acharenses/Equites/Nubes/Vespae/P...</td>\n",
       "      <td>Aristophanes / F.W. Hall / W.M. Geldart</td>\n",
       "      <td>5.00</td>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1922</td>\n",
       "      <td>Oxford University Press  USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>Melville and the politics of identity: From *K...</td>\n",
       "      <td>Julian Markels</td>\n",
       "      <td>3.33</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1993</td>\n",
       "      <td>University of Illinois Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>April  May und June</td>\n",
       "      <td>Elizabeth von Arnim</td>\n",
       "      <td>3.88</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>Insel  Frankfurt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>Dr No / Moonraker / Thunderball / From Russia ...</td>\n",
       "      <td>Ian Fleming</td>\n",
       "      <td>3.98</td>\n",
       "      <td>862</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1984</td>\n",
       "      <td>Heinemann-Octopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>V.S. Naipaul</td>\n",
       "      <td>Bruce Alvin King</td>\n",
       "      <td>2.00</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>Palgrave Macmillan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>The Baby Emergency (Tennengarrah Clinic #1)</td>\n",
       "      <td>Carol Marinelli</td>\n",
       "      <td>3.60</td>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>Mills &amp; Boon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>Animales No Se Visten  Los (Animals Should Def...</td>\n",
       "      <td>Judi Barrett / Ron Barrett</td>\n",
       "      <td>4.11</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1991</td>\n",
       "      <td>Live Oak Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>Ya Sé Que Te Quiero</td>\n",
       "      <td>Billy Crystal / Elizabeth Sayles</td>\n",
       "      <td>4.19</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>Rayo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>Roald Dahl: The Storyteller (Famous Lives)</td>\n",
       "      <td>Jason Hook</td>\n",
       "      <td>4.06</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>Hodder Wayland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>Les Nouvelles Recettes irrésistibles de Roald ...</td>\n",
       "      <td>Roald Dahl / Quentin Blake</td>\n",
       "      <td>4.14</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Gallimard Jeunesse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3133</th>\n",
       "      <td>Les Larmes d'Icare</td>\n",
       "      <td>Dan Simmons / Jean-Daniel Brèque</td>\n",
       "      <td>3.81</td>\n",
       "      <td>357</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1994</td>\n",
       "      <td>Denoël</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3235</th>\n",
       "      <td>City on the Seine: Paris in the Time of Richel...</td>\n",
       "      <td>Andrew P. Trout</td>\n",
       "      <td>4.17</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1996</td>\n",
       "      <td>Palgrave Macmillan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>A Book of Blue Flowers</td>\n",
       "      <td>Robert L. Geneve</td>\n",
       "      <td>3.83</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>Timber Press (OR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>A Streetcar Named Desire (SparkNotes Literatur...</td>\n",
       "      <td>SparkNotes</td>\n",
       "      <td>3.29</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>SparkNotes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>Premières Histoires</td>\n",
       "      <td>João Guimarães Rosa</td>\n",
       "      <td>4.33</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>Métailié</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4214</th>\n",
       "      <td>It's Only a Movie  Ingrid: Encounters on and O...</td>\n",
       "      <td>Alexander  Walker</td>\n",
       "      <td>3.80</td>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1989</td>\n",
       "      <td>Headline Book Publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>American Government: Continuity and Change  Te...</td>\n",
       "      <td>Karen  O'Connor / Larry J. Sabato</td>\n",
       "      <td>2.83</td>\n",
       "      <td>1088</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>Longman Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4856</th>\n",
       "      <td>La gata perdida = The Missing Cat (Las Aventur...</td>\n",
       "      <td>Chris L. Demarest / Berlitz Publishing Company</td>\n",
       "      <td>4.14</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>Berlitz Kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477</th>\n",
       "      <td>The Goon Show: Moriarty Where Are You?</td>\n",
       "      <td>NOT A BOOK</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>BBC Physical Audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5726</th>\n",
       "      <td>Math Fables</td>\n",
       "      <td>Greg Tang / Heather Cahoon</td>\n",
       "      <td>4.03</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1949</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781</th>\n",
       "      <td>The Best American Mystery Stories 2002</td>\n",
       "      <td>James Ellroy / Otto Penzler / John Biguenet / ...</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Mariner Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>Open House</td>\n",
       "      <td>Elizabeth Berg</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001</td>\n",
       "      <td>Turtleback Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>らんま½ 12</td>\n",
       "      <td>Rumiko Takahashi</td>\n",
       "      <td>4.06</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>小学館 [Shōgakukan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6396</th>\n",
       "      <td>The Deep (Dive Trilogy)</td>\n",
       "      <td>Gordon Korman</td>\n",
       "      <td>3.78</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>Turtleback Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6563</th>\n",
       "      <td>L'Orient  c'est l'Orient</td>\n",
       "      <td>T. Coraghessan Boyle</td>\n",
       "      <td>3.64</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1993</td>\n",
       "      <td>Grasset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6809</th>\n",
       "      <td>McGraw-Hill's SAT I (McGraw-Hill's SAT I)</td>\n",
       "      <td>Christopher   Black / Mark Anestis</td>\n",
       "      <td>3.67</td>\n",
       "      <td>890</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>McGraw-Hill Companies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>An Odyssey in Learning and Perception</td>\n",
       "      <td>Eleanor J. Gibson</td>\n",
       "      <td>4.50</td>\n",
       "      <td>654</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bradford Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6976</th>\n",
       "      <td>Wissenschaft der Logik: Die Lehre Vom Begriff ...</td>\n",
       "      <td>Georg Wilhelm Friedrich Hegel</td>\n",
       "      <td>4.78</td>\n",
       "      <td>337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1994</td>\n",
       "      <td>F. Meiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>Leadership Challenge</td>\n",
       "      <td>James M. Kouzes</td>\n",
       "      <td>4.05</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>Pfeiffer &amp; Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7178</th>\n",
       "      <td>Katy and the Big Snow (Book &amp; Cassette)</td>\n",
       "      <td>Virginia Lee Burton</td>\n",
       "      <td>4.24</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>Houghton Mifflin Harcourt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7212</th>\n",
       "      <td>Secretos De Familia</td>\n",
       "      <td>Julia Glass</td>\n",
       "      <td>3.57</td>\n",
       "      <td>528</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>Puzzle-Roca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7754</th>\n",
       "      <td>A Farewell to Arms?: Beyond the Good Friday Ag...</td>\n",
       "      <td>Adrian Guelke / Michael Cox / Fiona Stephen</td>\n",
       "      <td>4.33</td>\n",
       "      <td>624</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>Manchester University Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7872</th>\n",
       "      <td>Pacto Con un Demonio</td>\n",
       "      <td>Kim Harrison</td>\n",
       "      <td>4.39</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>HarperTorch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8055</th>\n",
       "      <td>Choo Choo (Carry Along Book &amp; Cassette Favorites)</td>\n",
       "      <td>Virginia Lee Burton</td>\n",
       "      <td>3.91</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1989</td>\n",
       "      <td>Houghton Mifflin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8475</th>\n",
       "      <td>Conversations with Bernard Malamud (Literary C...</td>\n",
       "      <td>J. Michael Lennon / Lawrence M. Lasher</td>\n",
       "      <td>4.00</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1991</td>\n",
       "      <td>University Press of Mississippi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8745</th>\n",
       "      <td>Does God Know How to Tie Shoes?</td>\n",
       "      <td>Nancy White Carlstrom / Lori McElrath-Eslick</td>\n",
       "      <td>4.20</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1997</td>\n",
       "      <td>William B. Eerdmans Publishing Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8766</th>\n",
       "      <td>Kopfüber ins Glück</td>\n",
       "      <td>Anna Maxted</td>\n",
       "      <td>3.54</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>Goldmann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841</th>\n",
       "      <td>The Visual Arts: A History</td>\n",
       "      <td>Hugh Honour / John Fleming</td>\n",
       "      <td>3.85</td>\n",
       "      <td>960</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Harry N. Abrams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8878</th>\n",
       "      <td>Operation Spy School (Adam Sharp  #4)</td>\n",
       "      <td>George E. Stanley / Guy Francis</td>\n",
       "      <td>3.80</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>Random House Books for Young Readers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8942</th>\n",
       "      <td>The Damnation Game</td>\n",
       "      <td>Clive Barker</td>\n",
       "      <td>3.82</td>\n",
       "      <td>374</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1989</td>\n",
       "      <td>Random House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9067</th>\n",
       "      <td>La Ley del Exito</td>\n",
       "      <td>Paramahansa Yogananda</td>\n",
       "      <td>4.44</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>Self-Realization Fellowship Publishers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9159</th>\n",
       "      <td>Mistaken Identity</td>\n",
       "      <td>Nayantara Sahgal</td>\n",
       "      <td>2.98</td>\n",
       "      <td>324</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>Harper Collins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202</th>\n",
       "      <td>Horizontal Gene Transfer</td>\n",
       "      <td>Michael Syvanen / Clarence I. Kado</td>\n",
       "      <td>4.00</td>\n",
       "      <td>445</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Academic Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9702</th>\n",
       "      <td>Super Fast Out of Control</td>\n",
       "      <td>Louis Sachar</td>\n",
       "      <td>3.75</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>Bloomsbury Publishing PLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9851</th>\n",
       "      <td>The American Campaign: U.S. Presidential Campa...</td>\n",
       "      <td>James E. Campbell</td>\n",
       "      <td>5.00</td>\n",
       "      <td>314</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>Texas A&amp;M University Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10168</th>\n",
       "      <td>Worlds of Wonder</td>\n",
       "      <td>Terry Pastor / Damon Knight / Philip K. Dick /...</td>\n",
       "      <td>4.13</td>\n",
       "      <td>368</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1988</td>\n",
       "      <td>Orion Publishing Co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266</th>\n",
       "      <td>Bill Gates: Computer Legend (Famous Lives)</td>\n",
       "      <td>Sara Barton-Wood</td>\n",
       "      <td>5.00</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001</td>\n",
       "      <td>Raintree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10313</th>\n",
       "      <td>Billy Budd  Sailor and Other Uncompleted Writi...</td>\n",
       "      <td>Herman Melville / Hershel Parker / G. Thomas T...</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>Northwestern Univ Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10446</th>\n",
       "      <td>After Collapse: The Regeneration of Complex So...</td>\n",
       "      <td>Glenn M. Schwartz</td>\n",
       "      <td>4.00</td>\n",
       "      <td>336</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>University of Arizona Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10517</th>\n",
       "      <td>Mission Possible</td>\n",
       "      <td>Kenneth H. Blanchard</td>\n",
       "      <td>3.44</td>\n",
       "      <td>242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>McGraw-Hill Companies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10853</th>\n",
       "      <td>A Cargo of Women: Susannah Watson and the Conv...</td>\n",
       "      <td>Babette Smith</td>\n",
       "      <td>3.58</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>Rosenberg Publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10866</th>\n",
       "      <td>Courageous Faith Through the Year</td>\n",
       "      <td>Bill Hybels / Keri Wyatt Kent</td>\n",
       "      <td>3.50</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>IVP Books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "264                                Lonely Planet Londres   \n",
       "525    American Government: Continuity and Change  Al...   \n",
       "526    Essentials of American and Texas Government: C...   \n",
       "624    Comoediae 1: Acharenses/Equites/Nubes/Vespae/P...   \n",
       "747    Melville and the politics of identity: From *K...   \n",
       "935                                  April  May und June   \n",
       "1110   Dr No / Moonraker / Thunderball / From Russia ...   \n",
       "1681                                        V.S. Naipaul   \n",
       "2051         The Baby Emergency (Tennengarrah Clinic #1)   \n",
       "2255   Animales No Se Visten  Los (Animals Should Def...   \n",
       "2956                                 Ya Sé Que Te Quiero   \n",
       "3006          Roald Dahl: The Storyteller (Famous Lives)   \n",
       "3007   Les Nouvelles Recettes irrésistibles de Roald ...   \n",
       "3133                                  Les Larmes d'Icare   \n",
       "3235   City on the Seine: Paris in the Time of Richel...   \n",
       "3240                              A Book of Blue Flowers   \n",
       "3349   A Streetcar Named Desire (SparkNotes Literatur...   \n",
       "3380                                 Premières Histoires   \n",
       "4214   It's Only a Movie  Ingrid: Encounters on and O...   \n",
       "4748   American Government: Continuity and Change  Te...   \n",
       "4856   La gata perdida = The Missing Cat (Las Aventur...   \n",
       "5477              The Goon Show: Moriarty Where Are You?   \n",
       "5726                                         Math Fables   \n",
       "5781              The Best American Mystery Stories 2002   \n",
       "6046                                          Open House   \n",
       "6087                                             らんま½ 12   \n",
       "6396                             The Deep (Dive Trilogy)   \n",
       "6563                            L'Orient  c'est l'Orient   \n",
       "6809           McGraw-Hill's SAT I (McGraw-Hill's SAT I)   \n",
       "6816               An Odyssey in Learning and Perception   \n",
       "6976   Wissenschaft der Logik: Die Lehre Vom Begriff ...   \n",
       "7002                                Leadership Challenge   \n",
       "7178             Katy and the Big Snow (Book & Cassette)   \n",
       "7212                                 Secretos De Familia   \n",
       "7754   A Farewell to Arms?: Beyond the Good Friday Ag...   \n",
       "7872                                Pacto Con un Demonio   \n",
       "8055   Choo Choo (Carry Along Book & Cassette Favorites)   \n",
       "8475   Conversations with Bernard Malamud (Literary C...   \n",
       "8745                     Does God Know How to Tie Shoes?   \n",
       "8766                                  Kopfüber ins Glück   \n",
       "8841                          The Visual Arts: A History   \n",
       "8878               Operation Spy School (Adam Sharp  #4)   \n",
       "8942                                  The Damnation Game   \n",
       "9067                                    La Ley del Exito   \n",
       "9159                                   Mistaken Identity   \n",
       "9202                            Horizontal Gene Transfer   \n",
       "9702                           Super Fast Out of Control   \n",
       "9851   The American Campaign: U.S. Presidential Campa...   \n",
       "10168                                   Worlds of Wonder   \n",
       "10266         Bill Gates: Computer Legend (Famous Lives)   \n",
       "10313  Billy Budd  Sailor and Other Uncompleted Writi...   \n",
       "10446  After Collapse: The Regeneration of Complex So...   \n",
       "10517                                   Mission Possible   \n",
       "10853  A Cargo of Women: Susannah Watson and the Conv...   \n",
       "10866                  Courageous Faith Through the Year   \n",
       "\n",
       "                                                 authors  average_rating  \\\n",
       "264        Lonely Planet / Sarah Johnstone / Tom Masters            4.03   \n",
       "525                    Karen  O'Connor / Larry J. Sabato            2.83   \n",
       "526                    Karen  O'Connor / Larry J. Sabato            3.50   \n",
       "624              Aristophanes / F.W. Hall / W.M. Geldart            5.00   \n",
       "747                                       Julian Markels            3.33   \n",
       "935                                  Elizabeth von Arnim            3.88   \n",
       "1110                                         Ian Fleming            3.98   \n",
       "1681                                    Bruce Alvin King            2.00   \n",
       "2051                                     Carol Marinelli            3.60   \n",
       "2255                          Judi Barrett / Ron Barrett            4.11   \n",
       "2956                    Billy Crystal / Elizabeth Sayles            4.19   \n",
       "3006                                          Jason Hook            4.06   \n",
       "3007                          Roald Dahl / Quentin Blake            4.14   \n",
       "3133                    Dan Simmons / Jean-Daniel Brèque            3.81   \n",
       "3235                                     Andrew P. Trout            4.17   \n",
       "3240                                    Robert L. Geneve            3.83   \n",
       "3349                                          SparkNotes            3.29   \n",
       "3380                                 João Guimarães Rosa            4.33   \n",
       "4214                                   Alexander  Walker            3.80   \n",
       "4748                   Karen  O'Connor / Larry J. Sabato            2.83   \n",
       "4856      Chris L. Demarest / Berlitz Publishing Company            4.14   \n",
       "5477                                          NOT A BOOK            4.43   \n",
       "5726                          Greg Tang / Heather Cahoon            4.03   \n",
       "5781   James Ellroy / Otto Penzler / John Biguenet / ...            3.58   \n",
       "6046                                      Elizabeth Berg            3.70   \n",
       "6087                                    Rumiko Takahashi            4.06   \n",
       "6396                                       Gordon Korman            3.78   \n",
       "6563                                T. Coraghessan Boyle            3.64   \n",
       "6809                  Christopher   Black / Mark Anestis            3.67   \n",
       "6816                                   Eleanor J. Gibson            4.50   \n",
       "6976                       Georg Wilhelm Friedrich Hegel            4.78   \n",
       "7002                                     James M. Kouzes            4.05   \n",
       "7178                                 Virginia Lee Burton            4.24   \n",
       "7212                                         Julia Glass            3.57   \n",
       "7754         Adrian Guelke / Michael Cox / Fiona Stephen            4.33   \n",
       "7872                                        Kim Harrison            4.39   \n",
       "8055                                 Virginia Lee Burton            3.91   \n",
       "8475              J. Michael Lennon / Lawrence M. Lasher            4.00   \n",
       "8745        Nancy White Carlstrom / Lori McElrath-Eslick            4.20   \n",
       "8766                                         Anna Maxted            3.54   \n",
       "8841                          Hugh Honour / John Fleming            3.85   \n",
       "8878                     George E. Stanley / Guy Francis            3.80   \n",
       "8942                                        Clive Barker            3.82   \n",
       "9067                               Paramahansa Yogananda            4.44   \n",
       "9159                                    Nayantara Sahgal            2.98   \n",
       "9202                  Michael Syvanen / Clarence I. Kado            4.00   \n",
       "9702                                        Louis Sachar            3.75   \n",
       "9851                                   James E. Campbell            5.00   \n",
       "10168  Terry Pastor / Damon Knight / Philip K. Dick /...            4.13   \n",
       "10266                                   Sara Barton-Wood            5.00   \n",
       "10313  Herman Melville / Hershel Parker / G. Thomas T...            3.11   \n",
       "10446                                  Glenn M. Schwartz            4.00   \n",
       "10517                               Kenneth H. Blanchard            3.44   \n",
       "10853                                      Babette Smith            3.58   \n",
       "10866                      Bill Hybels / Keri Wyatt Kent            3.50   \n",
       "\n",
       "       num_pages  ratings_count  text_reviews_count  publication_year  \\\n",
       "264          480              0                   0              2006   \n",
       "525          664              0                   0              2005   \n",
       "526          854              0                   0              2005   \n",
       "624          364              0                   0              1922   \n",
       "747          164              0                   0              1993   \n",
       "935           88              0                   0              1995   \n",
       "1110         862              0                   0              1984   \n",
       "1681         240              0                   0              2003   \n",
       "2051         285              0                   0              2004   \n",
       "2255          32              0                   0              1991   \n",
       "2956          40              0                   0              2006   \n",
       "3006          48              0                   0              2004   \n",
       "3007          64              0                   0              2002   \n",
       "3133         357              0                   0              1994   \n",
       "3235         288              0                   0              1996   \n",
       "3240         327              0                   0              2006   \n",
       "3349          96              0                   0              2002   \n",
       "3380         205              0                   0              1995   \n",
       "4214         320              0                   0              1989   \n",
       "4748        1088              0                   0              2005   \n",
       "4856          64              0                   0              2006   \n",
       "5477           2              0                   0              2005   \n",
       "5726          40              0                   0              1949   \n",
       "5781           1              0                   0              2002   \n",
       "6046           0              0                   0              2001   \n",
       "6087         176              0                   0              2002   \n",
       "6396         148              0                   0              2003   \n",
       "6563         370              0                   0              1993   \n",
       "6809         890              0                   0              2005   \n",
       "6816         654              0                   0              1994   \n",
       "6976         337              0                   0              1994   \n",
       "7002          50              0                   0              2004   \n",
       "7178          40              0                   0              1999   \n",
       "7212         528              0                   1              2006   \n",
       "7754         624              0                   0              2006   \n",
       "7872         512              0                   0              2008   \n",
       "8055          48              0                   0              1989   \n",
       "8475         156              0                   0              1991   \n",
       "8745          32              0                   0              1997   \n",
       "8766         512              0                   0              2003   \n",
       "8841         960              0                   0              2002   \n",
       "8878          44              0                   0              2003   \n",
       "8942         374              0                   0              1989   \n",
       "9067          32              0                   0              1998   \n",
       "9159         324              0                   0              2016   \n",
       "9202         445              0                   0              2002   \n",
       "9702         128              0                   0              2005   \n",
       "9851         314              0                   0              2000   \n",
       "10168        368              0                   0              1988   \n",
       "10266         48              0                   0              2001   \n",
       "10313       1016              0                   0              2017   \n",
       "10446        336              0                   0              2006   \n",
       "10517        242              0                   0              1999   \n",
       "10853        264              0                   0              2005   \n",
       "10866        327              0                   0              2004   \n",
       "\n",
       "                                    publisher  \n",
       "264                                Geoplaneta  \n",
       "525                  Longman Publishing Group  \n",
       "526                  Longman Publishing Group  \n",
       "624              Oxford University Press  USA  \n",
       "747              University of Illinois Press  \n",
       "935                          Insel  Frankfurt  \n",
       "1110                        Heinemann-Octopus  \n",
       "1681                       Palgrave Macmillan  \n",
       "2051                             Mills & Boon  \n",
       "2255                           Live Oak Media  \n",
       "2956                                     Rayo  \n",
       "3006                           Hodder Wayland  \n",
       "3007                       Gallimard Jeunesse  \n",
       "3133                                   Denoël  \n",
       "3235                       Palgrave Macmillan  \n",
       "3240                        Timber Press (OR)  \n",
       "3349                               SparkNotes  \n",
       "3380                                 Métailié  \n",
       "4214                 Headline Book Publishing  \n",
       "4748                 Longman Publishing Group  \n",
       "4856                             Berlitz Kids  \n",
       "5477                       BBC Physical Audio  \n",
       "5726                          Scholastic Inc.  \n",
       "5781                            Mariner Books  \n",
       "6046                         Turtleback Books  \n",
       "6087                        小学館 [Shōgakukan]  \n",
       "6396                         Turtleback Books  \n",
       "6563                                  Grasset  \n",
       "6809                    McGraw-Hill Companies  \n",
       "6816                            Bradford Book  \n",
       "6976                                F. Meiner  \n",
       "7002                       Pfeiffer & Company  \n",
       "7178                Houghton Mifflin Harcourt  \n",
       "7212                              Puzzle-Roca  \n",
       "7754              Manchester University Press  \n",
       "7872                              HarperTorch  \n",
       "8055                         Houghton Mifflin  \n",
       "8475          University Press of Mississippi  \n",
       "8745   William B. Eerdmans Publishing Company  \n",
       "8766                                 Goldmann  \n",
       "8841                          Harry N. Abrams  \n",
       "8878     Random House Books for Young Readers  \n",
       "8942                             Random House  \n",
       "9067   Self-Realization Fellowship Publishers  \n",
       "9159                           Harper Collins  \n",
       "9202                           Academic Press  \n",
       "9702                Bloomsbury Publishing PLC  \n",
       "9851               Texas A&M University Press  \n",
       "10168                     Orion Publishing Co  \n",
       "10266                                Raintree  \n",
       "10313                 Northwestern Univ Press  \n",
       "10446             University of Arizona Press  \n",
       "10517                   McGraw-Hill Companies  \n",
       "10853                    Rosenberg Publishing  \n",
       "10866                               IVP Books  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the rows with ratings_count == 0\n",
    "# We can't have rows for which at the same time ratings_count is equal to 0, but average_rating is different from 0  since the \n",
    "# average_rating is the average of all ratings based on the ratings_count.\n",
    "data2[(data2[\"ratings_count\"]==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11046, 8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rows with average_rating != 0 but ratings_count == 0 must be dropped\n",
    "data3 = data2[~(data2['ratings_count'] == 0)]\n",
    "# data3 contains only rows with average_rating != 0 and ratings_count != 0\n",
    "\n",
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>The 5 Love Languages / The 5 Love Languages Jo...</td>\n",
       "      <td>Gary Chapman</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>2005</td>\n",
       "      <td>Moody Publishers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>The Tragedy of Pudd'nhead Wilson</td>\n",
       "      <td>Mark Twain / Michael Prichard</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>Tantor Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>Murder by Moonlight &amp; Other Mysteries (New Adv...</td>\n",
       "      <td>NOT A BOOK</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>Simon  Schuster Audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>The Unfortunate Tobacconist &amp; Other Mysteries ...</td>\n",
       "      <td>NOT A BOOK</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>Simon &amp; Schuster Audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>The Da Vinci Code (Robert Langdon  #2)</td>\n",
       "      <td>Dan Brown / Paul Michael</td>\n",
       "      <td>3.84</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>16</td>\n",
       "      <td>2006</td>\n",
       "      <td>Random House Audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10030</th>\n",
       "      <td>The Chessmen of Mars (Barsoom #5)</td>\n",
       "      <td>Edgar Rice Burroughs / John Bolen</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0</td>\n",
       "      <td>5147</td>\n",
       "      <td>157</td>\n",
       "      <td>2005</td>\n",
       "      <td>Tantor Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10192</th>\n",
       "      <td>Fine Lines (One-Eyed Mack  #6)</td>\n",
       "      <td>Jim Lehrer</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1995</td>\n",
       "      <td>Random House Value Publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10624</th>\n",
       "      <td>Stowaway and Milk Run: Two Unabridged Stories ...</td>\n",
       "      <td>Mary Higgins Clark / Jan Maxwell</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>Simon &amp; Schuster Audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10926</th>\n",
       "      <td>The Mask of the Enchantress</td>\n",
       "      <td>Victoria Holt</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1981</td>\n",
       "      <td>Ivy Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11081</th>\n",
       "      <td>Treasury of American Tall Tales: Volume 1: Dav...</td>\n",
       "      <td>David Bromberg / Jay Ungar / Molly Mason / Gar...</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>2006</td>\n",
       "      <td>Listening Library (Audio)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "306    The 5 Love Languages / The 5 Love Languages Jo...   \n",
       "853                     The Tragedy of Pudd'nhead Wilson   \n",
       "1061   Murder by Moonlight & Other Mysteries (New Adv...   \n",
       "1064   The Unfortunate Tobacconist & Other Mysteries ...   \n",
       "1230              The Da Vinci Code (Robert Langdon  #2)   \n",
       "...                                                  ...   \n",
       "10030                  The Chessmen of Mars (Barsoom #5)   \n",
       "10192                     Fine Lines (One-Eyed Mack  #6)   \n",
       "10624  Stowaway and Milk Run: Two Unabridged Stories ...   \n",
       "10926                        The Mask of the Enchantress   \n",
       "11081  Treasury of American Tall Tales: Volume 1: Dav...   \n",
       "\n",
       "                                                 authors  average_rating  \\\n",
       "306                                         Gary Chapman            4.70   \n",
       "853                        Mark Twain / Michael Prichard            3.79   \n",
       "1061                                          NOT A BOOK            4.00   \n",
       "1064                                          NOT A BOOK            3.50   \n",
       "1230                            Dan Brown / Paul Michael            3.84   \n",
       "...                                                  ...             ...   \n",
       "10030                  Edgar Rice Burroughs / John Bolen            3.83   \n",
       "10192                                         Jim Lehrer            3.23   \n",
       "10624                   Mary Higgins Clark / Jan Maxwell            3.49   \n",
       "10926                                      Victoria Holt            3.85   \n",
       "11081  David Bromberg / Jay Ungar / Molly Mason / Gar...            3.86   \n",
       "\n",
       "       num_pages  ratings_count  text_reviews_count  publication_year  \\\n",
       "306            0             22                   4              2005   \n",
       "853            0              3                   0              2003   \n",
       "1061           0              7                   2              2006   \n",
       "1064           0             12                   1              2003   \n",
       "1230           0             91                  16              2006   \n",
       "...          ...            ...                 ...               ...   \n",
       "10030          0           5147                 157              2005   \n",
       "10192          0             17                   4              1995   \n",
       "10624          0             64                   2              1999   \n",
       "10926          0             21                   1              1981   \n",
       "11081          0             36                   9              2006   \n",
       "\n",
       "                           publisher  \n",
       "306                 Moody Publishers  \n",
       "853                     Tantor Media  \n",
       "1061           Simon  Schuster Audio  \n",
       "1064          Simon & Schuster Audio  \n",
       "1230              Random House Audio  \n",
       "...                              ...  \n",
       "10030                   Tantor Media  \n",
       "10192  Random House Value Publishing  \n",
       "10624         Simon & Schuster Audio  \n",
       "10926                      Ivy Books  \n",
       "11081      Listening Library (Audio)  \n",
       "\n",
       "[75 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze rows with num_pages == 0\n",
    "# We can't have rows for which at the same time num_pages == 0, but average_rating != 0 since at least pages of these books must be read before\n",
    "# giving a rating.\n",
    "data3[(data3[\"num_pages\"]==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10971, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rows with average_rating != 0 but num_pages == 0 must be dropped\n",
    "data4 = data3[~(data3[\"num_pages\"]==0)]\n",
    "# data4 contains only rows with average_rating != 0, ratings_count != 0 and num_pages != 0\n",
    "\n",
    "data4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10971.000000</td>\n",
       "      <td>10971.000000</td>\n",
       "      <td>1.097100e+04</td>\n",
       "      <td>10971.000000</td>\n",
       "      <td>10971.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.943235</td>\n",
       "      <td>339.064990</td>\n",
       "      <td>1.818883e+04</td>\n",
       "      <td>549.457205</td>\n",
       "      <td>2000.183939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294536</td>\n",
       "      <td>240.173871</td>\n",
       "      <td>1.132562e+05</td>\n",
       "      <td>2593.630924</td>\n",
       "      <td>8.200245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.780000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>1.130000e+02</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1998.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.960000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>7.810000e+02</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>2003.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.140000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>5.138000e+03</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>2005.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>6576.000000</td>\n",
       "      <td>4.597666e+06</td>\n",
       "      <td>94265.000000</td>\n",
       "      <td>2020.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       average_rating     num_pages  ratings_count  text_reviews_count  \\\n",
       "count    10971.000000  10971.000000   1.097100e+04        10971.000000   \n",
       "mean         3.943235    339.064990   1.818883e+04          549.457205   \n",
       "std          0.294536    240.173871   1.132562e+05         2593.630924   \n",
       "min          1.000000      1.000000   1.000000e+00            0.000000   \n",
       "25%          3.780000    197.000000   1.130000e+02           10.000000   \n",
       "50%          3.960000    302.000000   7.810000e+02           48.000000   \n",
       "75%          4.140000    416.000000   5.138000e+03          244.000000   \n",
       "max          5.000000   6576.000000   4.597666e+06        94265.000000   \n",
       "\n",
       "       publication_year  \n",
       "count      10971.000000  \n",
       "mean        2000.183939  \n",
       "std            8.200245  \n",
       "min         1900.000000  \n",
       "25%         1998.000000  \n",
       "50%         2003.000000  \n",
       "75%         2005.000000  \n",
       "max         2020.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unecessary data\n",
    "del data, data1, data2, data3\n",
    "\n",
    "# Rename data4 as data, then delete data4\n",
    "data = data4.copy(deep=True)\n",
    "del data4\n",
    "\n",
    "# We will be working with only one dataframe named data which is a clean version of our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_rating</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170746</td>\n",
       "      <td>0.040815</td>\n",
       "      <td>0.033764</td>\n",
       "      <td>-0.046053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_pages</th>\n",
       "      <td>0.170746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032975</td>\n",
       "      <td>0.035104</td>\n",
       "      <td>-0.022084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratings_count</th>\n",
       "      <td>0.040815</td>\n",
       "      <td>0.032975</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.865931</td>\n",
       "      <td>0.044832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_reviews_count</th>\n",
       "      <td>0.033764</td>\n",
       "      <td>0.035104</td>\n",
       "      <td>0.865931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.067372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication_year</th>\n",
       "      <td>-0.046053</td>\n",
       "      <td>-0.022084</td>\n",
       "      <td>0.044832</td>\n",
       "      <td>0.067372</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    average_rating  num_pages  ratings_count  \\\n",
       "average_rating            1.000000   0.170746       0.040815   \n",
       "num_pages                 0.170746   1.000000       0.032975   \n",
       "ratings_count             0.040815   0.032975       1.000000   \n",
       "text_reviews_count        0.033764   0.035104       0.865931   \n",
       "publication_year         -0.046053  -0.022084       0.044832   \n",
       "\n",
       "                    text_reviews_count  publication_year  \n",
       "average_rating                0.033764         -0.046053  \n",
       "num_pages                     0.035104         -0.022084  \n",
       "ratings_count                 0.865931          0.044832  \n",
       "text_reviews_count            1.000000          0.067372  \n",
       "publication_year              0.067372          1.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze correlations between quantitative variables\n",
    "data[['average_rating', 'num_pages', 'ratings_count', 'text_reviews_count', 'publication_year']].corr()\n",
    "\n",
    "# No high correlation were found ... (Correlation lesser than 0.5 or greater than -0.5)\n",
    "# Using only these quantitative variables to explain average_rating won't give us models which can perform so well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of data\n",
    "data_copy1 = data.copy(deep=True)\n",
    "data_copy2 = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the num_pages column\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "Scaler1 = StandardScaler()\n",
    "Scaler2 = MinMaxScaler()\n",
    "\n",
    "data_copy1[['num_pages']] = Scaler1.fit_transform(data_copy1[['num_pages']])\n",
    "data_copy2[['num_pages']] = Scaler2.fit_transform(data_copy2[['num_pages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.097100e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.922181e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000046e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.407649e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-5.915359e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.543327e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.203451e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.596960e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          num_pages\n",
       "count  1.097100e+04\n",
       "mean   4.922181e-17\n",
       "std    1.000046e+00\n",
       "min   -1.407649e+00\n",
       "25%   -5.915359e-01\n",
       "50%   -1.543327e-01\n",
       "75%    3.203451e-01\n",
       "max    2.596960e+01"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy1[['num_pages']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10971.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.051417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.036528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.029810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.045779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.063118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          num_pages\n",
       "count  10971.000000\n",
       "mean       0.051417\n",
       "std        0.036528\n",
       "min        0.000000\n",
       "25%        0.029810\n",
       "50%        0.045779\n",
       "75%        0.063118\n",
       "max        1.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy2[['num_pages']].describe()\n",
    "# data_copy2 is the better format to use since having positive values for num_pages is more understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transform all the qualitative data into quantitative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TF-IDF vectorizer\n",
    "publisher_tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the 'publisher' column\n",
    "data_copy2_publisher_tfidf = publisher_tfidf_vectorizer.fit_transform(data_copy2['publisher'])\n",
    "# Convert the TF-IDF matrices to DataFrames\n",
    "data_copy2_publisher_df = pd.DataFrame(data_copy2_publisher_tfidf.toarray(), columns=publisher_tfidf_vectorizer.get_feature_names_out())\n",
    "# data_copy2_publisher_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TF-IDF vectorizers\n",
    "title_tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the 'title' column\n",
    "data_copy2_title_tfidf = title_tfidf_vectorizer.fit_transform(data_copy2['title'])\n",
    "# Convert the TF-IDF matrices to DataFrames\n",
    "data_copy2_title_df = pd.DataFrame(data_copy2_title_tfidf.toarray(), columns=title_tfidf_vectorizer.get_feature_names_out())\n",
    "# data_copy2_title_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy3 = pd.concat([data_copy2.drop(['publisher', 'title'], axis=1), data_copy2_publisher_df, data_copy2_title_df], axis=1, join='inner')\n",
    "del data_copy3['authors']\n",
    "\n",
    "data_copy3 = pd.concat([data_copy3, data_copy2['authors']], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TF-IDF vectorizers\n",
    "authors_tfidf_vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>10</th>\n",
       "      <th>18</th>\n",
       "      <th>1976</th>\n",
       "      <th>1st</th>\n",
       "      <th>2000</th>\n",
       "      <th>...</th>\n",
       "      <th>朱學恆</th>\n",
       "      <th>林靜華</th>\n",
       "      <th>橋口</th>\n",
       "      <th>皇冠編譯組</th>\n",
       "      <th>神尾葉子</th>\n",
       "      <th>章博</th>\n",
       "      <th>維人</th>\n",
       "      <th>羅琳</th>\n",
       "      <th>荒川弘</th>\n",
       "      <th>趙丕慧</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.57</td>\n",
       "      <td>0.099011</td>\n",
       "      <td>2095690</td>\n",
       "      <td>27591</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.49</td>\n",
       "      <td>0.132167</td>\n",
       "      <td>2153167</td>\n",
       "      <td>29221</td>\n",
       "      <td>2004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.42</td>\n",
       "      <td>0.053384</td>\n",
       "      <td>6333</td>\n",
       "      <td>244</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.56</td>\n",
       "      <td>0.066008</td>\n",
       "      <td>2339585</td>\n",
       "      <td>36325</td>\n",
       "      <td>2004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.78</td>\n",
       "      <td>0.408973</td>\n",
       "      <td>41428</td>\n",
       "      <td>164</td>\n",
       "      <td>2004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10812</th>\n",
       "      <td>3.63</td>\n",
       "      <td>0.023270</td>\n",
       "      <td>406</td>\n",
       "      <td>45</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10813</th>\n",
       "      <td>3.77</td>\n",
       "      <td>0.024183</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10814</th>\n",
       "      <td>3.93</td>\n",
       "      <td>0.036958</td>\n",
       "      <td>1780</td>\n",
       "      <td>220</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10815</th>\n",
       "      <td>4.08</td>\n",
       "      <td>0.061293</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10816</th>\n",
       "      <td>3.55</td>\n",
       "      <td>0.028593</td>\n",
       "      <td>315</td>\n",
       "      <td>15</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10667 rows × 21453 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       average_rating  num_pages  ratings_count  text_reviews_count  \\\n",
       "0                4.57   0.099011        2095690               27591   \n",
       "1                4.49   0.132167        2153167               29221   \n",
       "2                4.42   0.053384           6333                 244   \n",
       "3                4.56   0.066008        2339585               36325   \n",
       "4                4.78   0.408973          41428                 164   \n",
       "...               ...        ...            ...                 ...   \n",
       "10812            3.63   0.023270            406                  45   \n",
       "10813            3.77   0.024183              7                   1   \n",
       "10814            3.93   0.036958           1780                 220   \n",
       "10815            4.08   0.061293             44                   7   \n",
       "10816            3.55   0.028593            315                  15   \n",
       "\n",
       "       publication_year   10   18  1976  1st  2000  ...  朱學恆  林靜華   橋口  皇冠編譯組  \\\n",
       "0                  2006  0.0  0.0   0.0  0.0   0.0  ...  0.0  0.0  0.0    0.0   \n",
       "1                  2004  0.0  0.0   0.0  0.0   0.0  ...  0.0  0.0  0.0    0.0   \n",
       "2                  2003  0.0  0.0   0.0  0.0   0.0  ...  0.0  0.0  0.0    0.0   \n",
       "3                  2004  0.0  0.0   0.0  0.0   0.0  ...  0.0  0.0  0.0    0.0   \n",
       "4                  2004  0.0  0.0   0.0  0.0   0.0  ...  0.0  0.0  0.0    0.0   \n",
       "...                 ...  ...  ...   ...  ...   ...  ...  ...  ...  ...    ...   \n",
       "10812              2006  0.0  0.0   0.0  0.0   0.0  ...  0.0  0.0  0.0    0.0   \n",
       "10813              2003  0.0  0.0   0.0  0.0   0.0  ...  0.0  0.0  0.0    0.0   \n",
       "10814              2003  0.0  0.0   0.0  0.0   0.0  ...  0.0  0.0  0.0    0.0   \n",
       "10815              1994  0.0  0.0   0.0  0.0   0.0  ...  0.0  0.0  0.0    0.0   \n",
       "10816              2002  0.0  0.0   0.0  0.0   0.0  ...  0.0  0.0  0.0    0.0   \n",
       "\n",
       "       神尾葉子   章博   維人   羅琳  荒川弘  趙丕慧  \n",
       "0       0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1       0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2       0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3       0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4       0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...     ...  ...  ...  ...  ...  ...  \n",
       "10812   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10813   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10814   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10815   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10816   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[10667 rows x 21453 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform the 'authors' column\n",
    "data_copy3_authors_tfidf = authors_tfidf_vectorizer.fit_transform(data_copy3['authors'])\n",
    "# Convert the TF-IDF matrices to DataFrames\n",
    "data_copy3_authors_df = pd.DataFrame(data_copy3_authors_tfidf.toarray(), columns=authors_tfidf_vectorizer.get_feature_names_out())\n",
    "# data_copy3_authors_df\n",
    "data_copy4 = pd.concat([data_copy3.drop(['authors'], axis=1), data_copy3_authors_df], axis=1, join='inner')\n",
    "data_copy4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for eventually more advanced models like NLP\n",
    "data[\"text\"] = data.apply(lambda row: \"the book {} written by {} and published by {} in {}\".format(\n",
    "    row[\"title\"], row[\"authors\"], row[\"publisher\"], row[\"publication_year\"]), axis=1)\n",
    "data['text2'] = data.apply(lambda row:\"the book {} written by {} and published by {} in {} with {} pages rated by {} persons and reviewed by {} people\".format(\n",
    "    row[\"title\"], row[\"authors\"], row[\"publisher\"], row[\"publication_year\"], row['num_pages'], row['ratings_count'], row['text_reviews_count']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the book Harry Potter and the Half-Blood Prince (Harry Potter  #6) written by J.K. Rowling / Mary GrandPré and published by Scholastic Inc. in 2006'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View \n",
    "data[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the book Harry Potter and the Half-Blood Prince (Harry Potter  #6) written by J.K. Rowling / Mary GrandPré and published by Scholastic Inc. in 2006 with 652 pages rated by 2095690 persons and reviewed by 27591 people'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View \n",
    "data[\"text2\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choose Models for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Regression Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_copy4\n",
    "\n",
    "# Split data into training and test \n",
    "target = data[['average_rating']]\n",
    "features = data[data.columns[~(data.columns.isin(target.columns))].tolist()]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=123)\n",
    "\n",
    "# To be sure, we're still working with dataframes for X_train, X_test and we have arrays for y_train and y_test\n",
    "X_train = pd.DataFrame(X_train) \n",
    "X_test = pd.DataFrame(X_test)\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "del target, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 20.1 TiB for an array with shape (7466, 369933601) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create all polynomial combination of features having a degree less or equal to two\u001b[39;00m\n\u001b[0;32m     10\u001b[0m poly \u001b[38;5;241m=\u001b[39m PolynomialFeatures(degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, interaction_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m X_train2 \u001b[38;5;241m=\u001b[39m \u001b[43mpoly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# X_train2 = poly.fit_transform(X_train[quant_variables])\u001b[39;00m\n\u001b[0;32m     12\u001b[0m X_test2 \u001b[38;5;241m=\u001b[39m poly\u001b[38;5;241m.\u001b[39mfit_transform(X_test) \u001b[38;5;66;03m# X_test2 = poly.fit_transform(X_test[quant_variables])\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Fit our linear regression model on the train, then evaluate the test\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\preprocessing\\_polynomial.py:507\u001b[0m, in \u001b[0;36mPolynomialFeatures.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    503\u001b[0m     XP \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mhstack(columns, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mtocsc()\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;66;03m# Do as if _min_degree = 0 and cut down array after the\u001b[39;00m\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# computation, i.e. use _n_out_full instead of n_output_features_.\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m     XP \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_out_full\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;66;03m# What follows is a faster implementation of:\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;66;03m# for i, comb in enumerate(combinations):\u001b[39;00m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;66;03m#     XP[:, i] = X[:, comb].prod(1)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m \n\u001b[0;32m    524\u001b[0m     \u001b[38;5;66;03m# degree 0 term\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_bias:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 20.1 TiB for an array with shape (7466, 369933601) and data type float64"
     ]
    }
   ],
   "source": [
    "# We will use only quantitative variables to predict avarage_rating\n",
    "# quant_variables = ['num_pages', 'ratings_count', 'text_reviews_count', 'publication_year']\n",
    "\n",
    "# In our linear model, we will take into account the interaction among the quant_variables since we\n",
    "# already know that taking independantly, they don't have enough informations to predict average_rating.\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create all polynomial combination of features having a degree less or equal to two\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_train2 = poly.fit_transform(X_train) # X_train2 = poly.fit_transform(X_train[quant_variables])\n",
    "X_test2 = poly.fit_transform(X_test) # X_test2 = poly.fit_transform(X_test[quant_variables])\n",
    "\n",
    "# Fit our linear regression model on the train, then evaluate the test\n",
    "model = LinearRegression()\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_score = model.score(X_train2, y_train)\n",
    "test_score = model.score(X_test2, y_test)\n",
    "print(f\"Training R^2 score: {train_score:.4f}\")\n",
    "print(f\"Testing R^2 score: {test_score:.4f}\")\n",
    "\n",
    "# We obtain a R-squared of 3.7% on the test set.\n",
    "# We will look for more advanced models in order to have better results.\n",
    "\n",
    "del X_train2, X_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 474. MiB for an array with shape (5973, 10395) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\joblib\\parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\joblib\\parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 721, in _fit_and_score\n    X_train, y_train = _safe_split(estimator, X, y, train)\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 155, in _safe_split\n    X_subset = _safe_indexing(X, indices)\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 353, in _safe_indexing\n    return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 195, in _pandas_indexing\n    return X.take(key, axis=axis)\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\pandas\\core\\generic.py\", line 4068, in take\n    new_data = self._mgr.take(\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 877, in take\n    return self.reindex_indexer(\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 670, in reindex_indexer\n    new_blocks = [\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 671, in <listcomp>\n    blk.take_nd(\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 1061, in take_nd\n    new_values = algos.take_nd(\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\pandas\\core\\array_algos\\take.py\", line 118, in take_nd\n    return _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n  File \"c:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\pandas\\core\\array_algos\\take.py\", line 158, in _take_nd_ndarray\n    out = np.empty(out_shape, dtype=dtype)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 474. MiB for an array with shape (5973, 10395) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[163], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m scorer \u001b[38;5;241m=\u001b[39m make_scorer(r2_score)\n\u001b[0;32m     23\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mrf, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39mscorer, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Get the best parameters and best score\u001b[39;00m\n\u001b[0;32m     27\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\acoue\\OneDrive\\Desktop\\RateMe\\RateMe\\myenv\\lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 474. MiB for an array with shape (5973, 10395) and data type float64"
     ]
    }
   ],
   "source": [
    "# We will use only quantitative variables to predict avarage_rating\n",
    "# quant_variables = ['num_pages', 'ratings_count', 'text_reviews_count', 'publication_year']\n",
    "X_train2 = X_train #  X_train2 = X_train[quant_variables]\n",
    "X_test2 = X_test # X_test2 = X_test[quant_variables]\n",
    "\n",
    "# We will use a RandomForest Regressor which is more advanced than our linear model with interactions\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "# Create a Random Forest regressor\n",
    "rf = RandomForestRegressor(random_state=123)\n",
    "\n",
    "# Define the parameter grid to search to optimize the hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 15],   # Maximum depth of the tree\n",
    "    'min_samples_split': [5, 10]  # Minimum number of samples required to split a node\n",
    "}\n",
    "\n",
    "# Perform Grid Search Cross-Validation\n",
    "scorer = make_scorer(r2_score)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring=scorer, n_jobs=-1)\n",
    "grid_search.fit(X_train2, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_r2 = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best r2_score:\", best_r2)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_r2 = r2_score(y_test, best_model.predict(X_test2))\n",
    "print(\"r2-score on Test Set using Best Model:\", test_r2)\n",
    "\n",
    "# The Random Forest gives a better result compared to the linear regression,\n",
    "# But to achieve this 9.6% r_squared which is an augmentation of 1.54 campared to the 3.7% r_squared,\n",
    "# we need to train at least 1000 trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. BERT Model for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create classes and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We treat this problem as a classification model : we need to determine the right cut to seperate average_rating\n",
    "# into different groups.\n",
    "\n",
    "# We decide to create two groups for average_rating :\n",
    "# the group with average_rating lesser than or equal to 4 will be qualified low_medium,\n",
    "# the group with average_rating greater than 4 will be qualified high.\n",
    "# This separation has been chosen in order to have similar number in each group.\n",
    "\n",
    "target = data[['average_rating']]\n",
    "features = data[data.columns[~(data.columns.isin(target.columns))].tolist()]\n",
    "Rating_category = target['average_rating'].apply(lambda x: 'high' if x>4 else 'low_medium')\n",
    "\n",
    "# Split data into training and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, Rating_category, test_size=0.3, random_state=123)\n",
    "\n",
    "# To be sure, we're still working with dataframes for X_train, X_test and we have arrays for y_train and y_test\n",
    "X_train = pd.DataFrame(X_train) \n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "del target, features, Rating_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distibution of low and high in the test data is : \n",
      " average_rating\n",
      "low_medium    0.568044\n",
      "high          0.431956\n",
      "Name: count, dtype: float64\n",
      "\n",
      "\n",
      "The distibution of low and high in the train data is : \n",
      " average_rating\n",
      "low_medium    0.575856\n",
      "high          0.424144\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verify if the proportion of low_medium and high is the same in y_train and in y_test\n",
    "print(\"The distibution of low and high in the test data is : \\n\"\n",
    "      , y_test.value_counts()/sum(y_test.value_counts()))\n",
    "print('\\n')\n",
    "print(\"The distibution of low and high in the train data is : \\n\"\n",
    "      , y_train.value_counts()/sum(y_train.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1-score on the training is : 0.529439716067725\n",
      "The F1-score on the test is : 0.5534189925263948\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression to predict upper or lower\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# We will use only quantitative variables to predict avarage_rating\n",
    "quant_variables = ['num_pages', 'ratings_count', 'text_reviews_count', 'publication_year']\n",
    "X_train2 = X_train[quant_variables]\n",
    "X_test2 = X_test[quant_variables]\n",
    "\n",
    "# We normalize each feature in X_train2 and X_test2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Transformer = StandardScaler().fit(X_train2)\n",
    "X_train3 = pd.DataFrame(Transformer.transform(X_train2))\n",
    "X_test3 = pd.DataFrame(Transformer.transform(X_test2))\n",
    "del X_train2, X_test2\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression(random_state=123)\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train3, y_train)\n",
    "\n",
    "# Calculate the f1-score on the training and the test set\n",
    "from sklearn.metrics import f1_score\n",
    "f1_train = f1_score(y_train, model.predict(X_train3), average='weighted')\n",
    "print(f\"The F1-score on the training is : {f1_train}\")\n",
    "f1_test = f1_score(y_test, model.predict(X_test3), average='weighted')\n",
    "print(f\"The F1-score on the test is : {f1_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1-score on the training is : 0.5358595347504272\n",
      "The F1-score on the test is : 0.5500117592064663\n"
     ]
    }
   ],
   "source": [
    "# Create a Logistic Regression model in which we take into account the interaction between the features\n",
    "model2 = LogisticRegression(random_state=123)\n",
    "\n",
    "# Create all polynomial combination of features having a degree less or equal to two\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_train4 = poly.fit_transform(X_train3)\n",
    "X_test4 = poly.fit_transform(X_test3)\n",
    "\n",
    "# Train the model using the training data\n",
    "model2.fit(X_train4, y_train)\n",
    "\n",
    "# Calculate the f1-score on the training and the test set\n",
    "f1_train = f1_score(y_train, model2.predict(X_train4), average='weighted')\n",
    "print(f\"The F1-score on the training is : {f1_train}\")\n",
    "f1_test = f1_score(y_test, model2.predict(X_test4), average='weighted')\n",
    "print(f\"The F1-score on the test is : {f1_test}\")\n",
    "\n",
    "# When we try to take into account the different interactions between variables, the F1-score of the new model\n",
    "# isn't better than this of the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 15, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Best F1-score: 0.6182291956189468\n",
      "F1-score on Test Set using Best Model: 0.6114765872676544\n"
     ]
    }
   ],
   "source": [
    "# We will use only quantitative variables to predict avarage_rating\n",
    "quant_variables = ['num_pages', 'ratings_count', 'text_reviews_count', 'publication_year']\n",
    "X_train2 = X_train[quant_variables]\n",
    "X_test2 = X_test[quant_variables]\n",
    "\n",
    "# We will use a RandomForest Classifier which is more advanced than the logistic regression with interactions\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Random Forest regressor\n",
    "rf = RandomForestClassifier(random_state=123)\n",
    "\n",
    "# Define the parameter grid to search to optimize the hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500, 1000],  # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 15],   # Maximum depth of the tree\n",
    "    'min_samples_split': [5, 10]  # Minimum number of samples required to split a node\n",
    "}\n",
    "\n",
    "# Perform Grid Search Cross-Validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_train3, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best F1-score:\", best_score)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model2 = grid_search.best_estimator_\n",
    "test_f1 = f1_score(y_test, best_model2.predict(X_test3), average='weighted')\n",
    "print(\"F1-score on Test Set using Best Model:\", test_f1)\n",
    "\n",
    "# Even for a classification problem, using an optimized random forest of 500 trees based on only the quantitative features \n",
    "# doesn't give us more than 62% of F1-score.\n",
    "# So, using the qualitative variables or trying to look for more quantitatives features through some links is necessary if we\n",
    "# want better models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
